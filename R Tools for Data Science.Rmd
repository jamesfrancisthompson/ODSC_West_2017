---
title: "R Tools for Data Science"
author: Joseph Rickert
output: html_notebook
---
Not only is R the very best language for statistical inference, it is rapidly evolving to accommodate the needs of data scientists. In this session, we will review some of the fundamental strengths of the R language and then discuss how R is integrating into the world of production level data science.

For the fourth year in a row, R has made it into the [top 10 list](https://spectrum.ieee.org/computing/software/the-2017-top-programming-languages) of the IEEE Spectrum Magazine's survey of general purpose programming languages. That R should be there at all is astounding! Much of R's popularity can be attributed to the growth of data science. 
<br/>   
![](IEEErank.png)

A recent [StackOverflow Post](https://stackoverflow.blog/2017/10/10/impressive-growth-r/) contained the following chart which shows the top twelve industries in the United States and the United Kingdom that generated R related questions. 

![](visitsByIndustry.png)

The chart clearly shows that after Academia, R traffic originating from the Healthcare dominates the traffic by generated by other industries.

Moreover, another chart published in the same post shows R traffic is generally growing faster in industries where it was already popular. Academia and Healthcare are pulling away from the pack.

![](growthByIndustry.png)


## *________________________________________________________________________________________* 

## Our Agenda 
After a short discussion of why the R Language itself is particularly well suited to data science, we will look at examples of R tools in six categories: 
* Manipulating Data
* Accessing Data
* Visualizing Data
* Machine Learning
* Accessing Big Data Platforms
* Sharing Results

** Agenda Details **
* The R Language itself (15 min)
    *    A very brief history
    *    The Structure of R
    *    The R Package System
* Tools for Manipulating Data (20 min)
    *    The Tidyverse
* Tools for Accessing Data    (5 min)
    *    Accessing Databases
* Tools for Visualizing Data  (5 min)
* Machine Learning Algorithms (10 min)
    *    The caret package
* Tools for Big Data Platforms (30 min)
    *    Spark and Sparklyr
    *    Keras and Tensor Flow
* Tools for Sharing Results   (5 min)
    *    RMakdown
    *    Shiny
<br/>

## *________________________________________________________________________________________* 


### A Very Brief History of R

To appreciate this R's growth, consider that R evolved from a relatively modest effort to develop an interface to Fortran statistical routines to full featured language that provides access to the world's richest repository of statistical and machine learning algorithms. The following dates outline a short history of how this happened.
           
![](Rhistory.png)
<br/>         
          
So R is a direct descendant of the S Language developed at Bell Labs by John Chambers, Rick Becker and other, but was also heavily influenced by Lisp and Scheme. Robert Gentleman and Ross Ihaka introduced Lisp like semantics to an ideas such as lexical scoping.

## *________________________________________________________________________________________* 

### The Structure of the R Language
####R is all about "Flow"

The R language was designed to enhance the experience of data exploration and statistical inference. The original intent of the S designers to make a convenient interface for their high powered Fortran evolved into a project to make R a full-featured language for statistical inference and modeling. People productivity is at the core of the R experience. As John Chambers puts it in his book [Extending R](https://www.amazon.com/Extending-R-John-M-Chambers/dp/1138469270/ref=sr_1_1?ie=UTF8&qid=1506717425&sr=8-1&keywords=extending+r), **“One of the attractions of R has always been the ability to compute an interesting result quickly."** 
Ease of use and the ability to stay in the "flow" of an analysis is more important than computational efficiency.

**R is a functional, object-based language.** The three basic principles underlying R are:    
*  Everything that exists in R is an object    
*  Everything that happens in R is a function call    
*  Interfaces to other software are part of R    

Chambers elaborates on this last point as follows: **"A key motivation for the original S remains important now: to give easy access to the best computations for understanding data."** We see several examples of the efforts R developers put into connecting to the best things out there later in this presentation.

#### Objects and Functions
The interplay of objects and functions in R is apparent in the way in which data and statistical models are packaged. For example consider a simple linear model:

```{r}
x <- rnorm(100)
y <- rnorm(100)

reg <- lm(y ~ x)
summary(reg)
```

As the summary above indicates, R only returns model outputs when you ask for them and methods for generating summaries tend to be parsimonious. The model object `reg` packages quite a bit of information about the model.

```{r}
str(reg,give.attr=FALSE)

```


**Functions in R can call other functions.** This feature is well adapted to writing functions to estimate maximum likelihood and other statistical algorithms.

```{r}
# Two different rounding options
round(pi,4); signif(pi,4)     
# This is the way to have one function call another function
jmean <- function(x,FUN,...){
               m <- FUN(sum(x)/length(x),...)
               return(m)}

x <- rnorm(100)
jmean(x,round,4); jmean(x, signif,4)

```

#### Missing Values
A significant advantage that R has over other scripting languages is that it has an inate mechanism for dealing with missing values: NA**
```{r}
z <- c(1:3, NA)
z
is.na(z)

```

Not only is is easy to identify and work with missing values in R, like here:
```{r}
a <- 1:5
b <- rnorm(5)
c <- LETTERS[1:5]
dF <- data.frame(a,b,c)
dF$b[3] <- NA; dF$c[4] <- NA
dF
na.omit(dF)
```
but, R also has several packages devoted to missing value imputation including:     
* [BaBooN](https://cran.r-project.org/package=BaBooN)    
* [mi](https://cran.r-project.org/package=mi)     
* [mice](https://cran.r-project.org/package=mice)     
* [missMDA](https://CRAN.R-project.org/package=missMDA)    
* [VIM](https://cran.r-project.org/package=VIM)    

#### The Data Frame
A data frame is a list in which each row may represent an observation and each column a variable, is a natural data structure for statistical analysis. This consistent, ubiquitious data structure **provides R with a big technical advantage**. This is no accident. Both Robert Gentleman and Ross Ihaka believed in [Niklaus Wirth’s](https://en.wikipedia.org/wiki/Niklaus_Wirth) dictum: [`algorithms + data structures = programs`](https://en.wikipedia.org/wiki/Algorithms_%2B_Data_Structures_%3D_Programs).

## *________________________________________________________________________________________* 

## The R Package System
There are over 11,500 packages on CRAN, R's central repository. The algorithms in these packages comprise a transparent, documented statistical resource of great value. Searching the through CRAN is a challenge, but the [CRAN Task Views](https://cran.r-project.org/web/views/) lists of R packages, curated by experts and organized by application area mitigate the problem.

![](taskviews.png)


### The Tidyverse
Another great advantage of R is its ability to facilitate the construction of Domain Specific Languages. As Shiny's creator Joe Cheng puts it [(Interiew with Joe Cheng)](https://rviews.rstudio.com/2017/01/04/interview-with-joe-cheng/):

> If you look beyond the syntax, R really is conceptually very much like Lisp in a lot of ways. One of those ways is that it makes it very, very easy to compute on the programming language itself. ...

> I think day-to-day, R programmers probably don’t think about these things, but the elegant, terse syntax of dplyr and the pipe operator are possible because of how malleable a language R is and how great it is for writing DSLs in it.

> Personally, one of my pet peeves during these language wars is when people say that one of the dierences between say Python or Julia and R is that R is a DSL for stats, whereas these other things are general purpose languages. R is not a DSL. It’s a language for writing DSLs, which is something that’s altogether more powerful. I actually think that Julia has many of these same characteristics, but Python, even though it obviously has its own strengths, certainly doesn’t share that same level of fexibility.

One way to think of the [Tidyverse](https://www.tidyverse.org/) is that it is a DSL for doing data science. The packages that comprise the tidyverse offer a consistent, integrated set of functions for implementing the canonical data science workflow.

![](tidyverse.png)
<br/>
<br/>
The name "tidyverse" comes from the concept of "tidy" data which Hadley Wickham, the tedyverse's principal architect and implementor, describes as follows: 

> [Tidy data](http://vita.had.co.nz/papers/tidy-data.pdf) is a standard way of mapping the meaning of a dataset to its structure. A dataset is messy or tidy depending on how rows, columns and tables are matched up with observations, variables and types. In tidy data: (1) Each variable forms a column, (2) Each observation forms a row and (3) Each type of observational unit forms a table.

#### Manipulating and Transforming Data
The `dplyr` package which provides a consistent set of verbs for manipulating data provides the backbone structure for the import, tydy and transform sequence of the data science workflow. The basic `dplyr` verbs are:
* `filter()` to select cases based on their values    
* `arrange()` to reorder the cases   
* `select()` and `rename()` to select variables based on their names   
* `mutate()` and `transmute()` to add new variables that are functions of existing variables    
* `summarise()`  to condense multiple values to a single value    
* `sample_n()` and `sample_frac()`  to take random samples

Here we will look at a few examples of working with these verbs using nycflights13::flights, a dataset containing all 336776 flights that departed from New York City in 2013. The data comes from the [US Bureau of Transportation Statistics](https://www.transtats.bts.gov/DatabaseInfo.asp?DB_ID=120&Link=0), and is documented in ?nycflights13

```{r}
#install.packages("tidyverse","nycflights13")
library(nycflights13)
library(tidyverse)
data(flights)
flights
```


In this first example, we use `dplyr::filter()` to fetch flights that arrived early in January.

```{r}
# filter by departure delay and print the first few records
flights %>% filter(month == 1, dep_delay < 0)
```

`arrange()` reorders rows. It takes a data frame, and a set of column names (or more complicated expressions) to order by. If you provide more than one column name, each additional column will be used to break ties in the values of preceding columns:

```{r}
arrange(flights, sched_dep_time, dep_delay)

```

`select()` allows you to select columns by name. Here we sleect the columns `origin`, `dest` and `air_time`, and then sort by `air_time` from largest to smallest.

```{r}
select(flights, origin, dest, air_time) %>% arrange(desc(air_time))
```

`mutate()` allows you to add a new column to a data frame. Here we add in a column for airspeed.

```{r}
mutate(flights,
  gain = arr_delay - dep_delay,
  speed = distance / air_time * 60
)
```

`summarise()` will compute a statistic and collapse a data frame into a single row.

```{r}
summarise(flights,
  delay = mean(dep_delay, na.rm = TRUE)
)
```

The `group_by()` functions allows you use the verbs discussed above on groups of observations in a dataset. Here we compute mean distance and arrival delay for each individual plane and plot the results.

```{r}
delay <- flights %>% 
  group_by(tailnum) %>%
  summarise(count = n(), dist = mean(distance), delay = mean(arr_delay)) %>%
  filter(count > 20, dist < 2000, !is.na(delay))

# plot delays
library(ggplot2)
ggplot(delay, aes(dist, delay)) +
  geom_point(aes(size = count), alpha = 1/2) +
  geom_smooth() +
  scale_size_area(max_size = 2)
```



For a more complete presentation using this data see the [dplyr vignette](https://cran.r-project.org/web/packages/dplyr/vignettes/dplyr.html).

#### Importing from Databases
With the aid of the `DBI` package, `dply` is able to import data from several different open source data bases including *MySQL* and *MariaDB* with the [`RMariaDB`](https://github.com/rstats-db/RMariaDB) package, *Postgres* and *Redshift* with [`RPostgressSQL`](https://cran.r-project.org/web/packages/RPostgreSQL/), and *SQLite* with [`RSQLite`](https://cran.r-project.org/web/packages/RSQLite/index.html). 

The following simple example illustrates working with SQLite. Again, we use the nycflights13 data set to create a SQLite table with 1,000 rows and 19 columns.
```{r}
library(tidyverse)
library(DBI)
library(RSQLite)
library(dbplyr)
library(nycflights13)

con <- DBI::dbConnect(RSQLite::SQLite(), path = ":memory:")

copy_to(con, nycflights13::flights, "flights",
  temporary = FALSE, 
  indexes = list(
    c("year", "month", "day"), 
    "carrier", 
    "tailnum",
    "dest"
  )
)

flights_db <- tbl(con, "flights")
flights_db 
```

Now that we have established a connection to the database table, we can use the normal dplyr verbs to query the data.

```{r}
# List departure delay and arrival delay for year month and day.
flights_db %>% select(year:day, dep_delay, arr_delay)

# Find records with departure delay greater than 240 minutes.
flights_db %>% filter(dep_delay > 240)

# Summarize mean departure delay by destination airport
flights_db %>% 
  group_by(dest) %>%
  summarise(delay = mean(dep_time))

# disconnect from database
dbDisconnect(con)
```

`dplyr` can also be use with data stored in commerical databases. The [`bigquery`](https://CRAN.R-project.org/package=bigrquery) enables working with data in Google's [BigQuery](https://cloud.google.com/bigquery/) platform, and the [`odbc`](package) permits many commercial databases to be used as a `dplyr` backend through the open database connectivity protocol,

For additional information on basic `dbplyr` functionality see the [vignette](https://cran.r-project.org/web/packages/dbplyr/vignettes/dbplyr.html). For an advanced treatment using `dplyr` with commercial grade databases see the three blog posts from Edgar Ruiz:    
* [Databases using R](https://rviews.rstudio.com/2017/05/17/databases-using-r/)   
* [Visualizations with R and Databases](https://rviews.rstudio.com/2017/08/16/visualizations-with-r-and-databases/)   
* [Enterprise-ready dashboards with Shiny and databases](https://rviews.rstudio.com/2017/09/20/dashboards-with-r-and-databases/).

**Be sure to catch Edgar's talk at this conference tomorrow.**

## *________________________________________________________________________________________* 



